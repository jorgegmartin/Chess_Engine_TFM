{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import chess.engine\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.utils as utils\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "import tensorflow.keras.callbacks as callbacks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17859364810849125948\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6254755840\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 13242754520659391415\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 2070 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check GPU is available\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(conv_size, conv_depth):\n",
    "  board3d = layers.Input(shape=(14, 8, 8))\n",
    "\n",
    "  # adding the convolutional layers\n",
    "  x = board3d\n",
    "  for _ in range(conv_depth):\n",
    "    x = layers.Conv2D(filters=conv_size, kernel_size=3, padding='same', activation='relu', data_format='channels_first')(x)\n",
    "  x = layers.Flatten()(x)\n",
    "  x = layers.Dense(64, 'relu')(x)\n",
    "  x = layers.Dense(1, 'sigmoid')(x)\n",
    "\n",
    "  return models.Model(inputs=board3d, outputs=x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual model\n",
    "\n",
    "Skip connections (residual network) will likely improve the model for deeper connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_residual(conv_size, conv_depth):\n",
    "  board3d = layers.Input(shape=(14, 8, 8))\n",
    "\n",
    "  # adding the convolutional layers\n",
    "  x = layers.Conv2D(filters=conv_size, kernel_size=3, padding='same', data_format='channels_first')(board3d)\n",
    "  for _ in range(conv_depth):\n",
    "    previous = x\n",
    "    x = layers.Conv2D(filters=conv_size, kernel_size=3, padding='same', data_format='channels_first')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(filters=conv_size, kernel_size=3, padding='same', data_format='channels_first')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Add()([x, previous])\n",
    "    x = layers.Activation('relu')(x)\n",
    "  x = layers.Flatten()(x)\n",
    "  x = layers.Dense(1, 'sigmoid')(x)\n",
    "\n",
    "  return models.Model(inputs=board3d, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_easy = build_model(64, 5)\n",
    "model_medium = build_model(64, 5)\n",
    "model_hard = build_model(64, 5)\n",
    "model_re = build_model_residual(64, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=673x154>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import visualkeras\n",
    "from PIL import ImageFont\n",
    "visualkeras.layered_view(model_easy, legend=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=1573x154>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualkeras.layered_view(model_re, legend=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training ⬇️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500000, 14, 8, 8)\n",
      "(1500000,)\n"
     ]
    }
   ],
   "source": [
    "def get_dataset(model_selected):\n",
    "\tmodel_path_d3 = '../data/random_generated/random_boards_d3.npz'\n",
    "\tmodel_path_d6 = '../data/random_generated/random_boards_d6.npz'\n",
    "\tmodel_path_d10 = '../data/random_generated/random_boards_d10.npz'\n",
    "\n",
    "\tmodel_path = {\n",
    "\t\t'd3': model_path_d3,\n",
    "\t\t'd6': model_path_d6,\n",
    "\t\t'd10': model_path_d10,\n",
    "\t}\n",
    "\t\n",
    "\tcontainer = np.load(model_path[model_selected], allow_pickle=True)\n",
    "\tb, v, f = container['board_matrix'], container['eval'], container['board_fen']\n",
    "\tv[v == None] = 0\n",
    "\tv = np.asarray(v / abs(v).max() / 2 + 0.5, dtype=np.float32) # normalization (0 - 1)\n",
    "\treturn b, v\n",
    "\n",
    "\n",
    "x_train, y_train = get_dataset('d10')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EASY MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 14, 8, 8)]        0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 64, 8, 8)          8128      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 64, 8, 8)          36928     \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 64, 8, 8)          36928     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 64, 8, 8)          36928     \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 64, 8, 8)          36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                262208    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 418,113\n",
      "Trainable params: 418,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_easy.compile(optimizer=optimizers.Adam(5e-4), loss='mean_squared_error')\n",
    "model_easy.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 6.0864e-04 - val_loss: 0.0014 - lr: 5.0000e-04\n",
      "Epoch 2/1000\n",
      "660/660 [==============================] - 15s 23ms/step - loss: 6.2279e-04 - val_loss: 0.0014 - lr: 5.0000e-04\n",
      "Epoch 3/1000\n",
      "660/660 [==============================] - 15s 23ms/step - loss: 6.2028e-04 - val_loss: 0.0014 - lr: 5.0000e-04\n",
      "Epoch 4/1000\n",
      "660/660 [==============================] - 15s 23ms/step - loss: 6.2102e-04 - val_loss: 0.0014 - lr: 5.0000e-04\n",
      "Epoch 5/1000\n",
      "660/660 [==============================] - 15s 23ms/step - loss: 6.1354e-04 - val_loss: 0.0014 - lr: 5.0000e-04\n",
      "Epoch 6/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 6.1145e-04 - val_loss: 0.0014 - lr: 5.0000e-04\n",
      "Epoch 7/1000\n",
      "660/660 [==============================] - 15s 23ms/step - loss: 6.0406e-04 - val_loss: 0.0014 - lr: 5.0000e-04\n",
      "Epoch 8/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 5.9427e-04 - val_loss: 0.0015 - lr: 5.0000e-04\n",
      "Epoch 9/1000\n",
      "660/660 [==============================] - 15s 23ms/step - loss: 5.9319e-04 - val_loss: 0.0014 - lr: 5.0000e-04\n",
      "Epoch 10/1000\n",
      "660/660 [==============================] - 15s 23ms/step - loss: 5.9230e-04 - val_loss: 0.0015 - lr: 5.0000e-04\n",
      "Epoch 11/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 5.8373e-04 - val_loss: 0.0015 - lr: 5.0000e-04\n",
      "Epoch 12/1000\n",
      "660/660 [==============================] - 14s 22ms/step - loss: 5.1069e-04 - val_loss: 0.0014 - lr: 5.0000e-05\n",
      "Epoch 13/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 4.9070e-04 - val_loss: 0.0014 - lr: 5.0000e-05\n",
      "Epoch 14/1000\n",
      "660/660 [==============================] - 16s 25ms/step - loss: 4.8264e-04 - val_loss: 0.0015 - lr: 5.0000e-05\n",
      "Epoch 15/1000\n",
      "660/660 [==============================] - 17s 25ms/step - loss: 4.7759e-04 - val_loss: 0.0015 - lr: 5.0000e-05\n",
      "Epoch 16/1000\n",
      "660/660 [==============================] - 17s 25ms/step - loss: 4.7350e-04 - val_loss: 0.0015 - lr: 5.0000e-05\n",
      "Epoch 17/1000\n",
      "660/660 [==============================] - 15s 23ms/step - loss: 4.6976e-04 - val_loss: 0.0015 - lr: 5.0000e-05\n",
      "Epoch 18/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 4.6684e-04 - val_loss: 0.0015 - lr: 5.0000e-05\n",
      "Epoch 19/1000\n",
      "660/660 [==============================] - 15s 22ms/step - loss: 4.6413e-04 - val_loss: 0.0015 - lr: 5.0000e-05\n",
      "Epoch 20/1000\n",
      "660/660 [==============================] - 16s 25ms/step - loss: 4.6163e-04 - val_loss: 0.0015 - lr: 5.0000e-05\n",
      "Epoch 21/1000\n",
      "660/660 [==============================] - 16s 25ms/step - loss: 4.5972e-04 - val_loss: 0.0015 - lr: 5.0000e-05\n",
      "Epoch 22/1000\n",
      "660/660 [==============================] - 16s 25ms/step - loss: 4.5701e-04 - val_loss: 0.0015 - lr: 5.0000e-05\n",
      "Epoch 23/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 4.5510e-04 - val_loss: 0.0015 - lr: 5.0000e-05\n",
      "Epoch 24/1000\n",
      "660/660 [==============================] - 14s 22ms/step - loss: 4.4407e-04 - val_loss: 0.0015 - lr: 5.0000e-06\n",
      "Epoch 25/1000\n",
      "660/660 [==============================] - 14s 22ms/step - loss: 4.4306e-04 - val_loss: 0.0015 - lr: 5.0000e-06\n",
      "Epoch 26/1000\n",
      "660/660 [==============================] - 15s 23ms/step - loss: 4.4265e-04 - val_loss: 0.0015 - lr: 5.0000e-06\n",
      "Epoch 27/1000\n",
      "660/660 [==============================] - 14s 22ms/step - loss: 4.4231e-04 - val_loss: 0.0015 - lr: 5.0000e-06\n",
      "Epoch 28/1000\n",
      "660/660 [==============================] - 14s 22ms/step - loss: 4.4205e-04 - val_loss: 0.0015 - lr: 5.0000e-06\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = get_dataset('d3')\n",
    "model_easy.fit(x_train, y_train,\n",
    "          batch_size=2048,\n",
    "          epochs=1000,\n",
    "          verbose=1,\n",
    "          validation_split=0.1,\n",
    "          callbacks=[callbacks.ReduceLROnPlateau(monitor='loss', patience=10),\n",
    "                     callbacks.EarlyStopping(monitor='loss', patience=15, min_delta=1e-4)])\n",
    "\n",
    "model_easy.save('./engine_models/depth3_model.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tiempo de ejecucion = 16 mins 20 seg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MEDIUM MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 14, 8, 8)]        0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 64, 8, 8)          8128      \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 64, 8, 8)          36928     \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 64, 8, 8)          36928     \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 64, 8, 8)          36928     \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 64, 8, 8)          36928     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                262208    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 418,113\n",
      "Trainable params: 418,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_medium.compile(optimizer=optimizers.Adam(5e-4), loss='mean_squared_error')\n",
    "model_medium.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "660/660 [==============================] - 19s 28ms/step - loss: 0.0017 - val_loss: 0.0013 - lr: 5.0000e-04\n",
      "Epoch 2/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0013 - val_loss: 0.0012 - lr: 5.0000e-04\n",
      "Epoch 3/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0012 - val_loss: 0.0012 - lr: 5.0000e-04\n",
      "Epoch 4/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0012 - val_loss: 0.0012 - lr: 5.0000e-04\n",
      "Epoch 5/1000\n",
      "660/660 [==============================] - 18s 28ms/step - loss: 0.0012 - val_loss: 0.0012 - lr: 5.0000e-04\n",
      "Epoch 6/1000\n",
      "660/660 [==============================] - 18s 28ms/step - loss: 0.0011 - val_loss: 0.0012 - lr: 5.0000e-04\n",
      "Epoch 7/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0011 - val_loss: 0.0012 - lr: 5.0000e-04\n",
      "Epoch 8/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0011 - val_loss: 0.0012 - lr: 5.0000e-04\n",
      "Epoch 9/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0011 - val_loss: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 10/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0011 - val_loss: 0.0012 - lr: 5.0000e-04\n",
      "Epoch 11/1000\n",
      "660/660 [==============================] - 19s 28ms/step - loss: 0.0011 - val_loss: 0.0012 - lr: 5.0000e-04\n",
      "Epoch 12/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0011 - val_loss: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 13/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0010 - val_loss: 0.0012 - lr: 5.0000e-04\n",
      "Epoch 14/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0010 - val_loss: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 15/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0010 - val_loss: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 16/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 9.8837e-04 - val_loss: 0.0011 - lr: 5.0000e-04\n",
      "Epoch 17/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 9.7692e-04 - val_loss: 0.0012 - lr: 5.0000e-04\n",
      "Epoch 18/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 9.5644e-04 - val_loss: 0.0012 - lr: 5.0000e-04\n",
      "Epoch 19/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 9.3819e-04 - val_loss: 0.0012 - lr: 5.0000e-04\n",
      "Epoch 20/1000\n",
      "660/660 [==============================] - 19s 29ms/step - loss: 9.2120e-04 - val_loss: 0.0012 - lr: 5.0000e-04\n",
      "Epoch 21/1000\n",
      "660/660 [==============================] - 19s 28ms/step - loss: 9.0314e-04 - val_loss: 0.0012 - lr: 5.0000e-04\n",
      "Epoch 22/1000\n",
      "660/660 [==============================] - 20s 30ms/step - loss: 8.9072e-04 - val_loss: 0.0012 - lr: 5.0000e-04\n",
      "Epoch 23/1000\n",
      "660/660 [==============================] - 18s 28ms/step - loss: 8.7180e-04 - val_loss: 0.0012 - lr: 5.0000e-04\n",
      "Epoch 24/1000\n",
      "660/660 [==============================] - 18s 28ms/step - loss: 8.5444e-04 - val_loss: 0.0012 - lr: 5.0000e-04\n",
      "Epoch 25/1000\n",
      "660/660 [==============================] - 19s 28ms/step - loss: 8.4308e-04 - val_loss: 0.0012 - lr: 5.0000e-04\n",
      "Epoch 26/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 8.2944e-04 - val_loss: 0.0012 - lr: 5.0000e-04\n",
      "Epoch 27/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 8.1503e-04 - val_loss: 0.0013 - lr: 5.0000e-04\n",
      "Epoch 28/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 8.0557e-04 - val_loss: 0.0014 - lr: 5.0000e-04\n",
      "Epoch 29/1000\n",
      "660/660 [==============================] - 19s 29ms/step - loss: 7.8947e-04 - val_loss: 0.0012 - lr: 5.0000e-04\n",
      "Epoch 30/1000\n",
      "660/660 [==============================] - 19s 29ms/step - loss: 7.8500e-04 - val_loss: 0.0013 - lr: 5.0000e-04\n",
      "Epoch 31/1000\n",
      "660/660 [==============================] - 19s 28ms/step - loss: 7.7163e-04 - val_loss: 0.0013 - lr: 5.0000e-04\n",
      "Epoch 32/1000\n",
      "660/660 [==============================] - 18s 28ms/step - loss: 7.5951e-04 - val_loss: 0.0013 - lr: 5.0000e-04\n",
      "Epoch 33/1000\n",
      "660/660 [==============================] - 18s 28ms/step - loss: 7.5062e-04 - val_loss: 0.0013 - lr: 5.0000e-04\n",
      "Epoch 34/1000\n",
      "660/660 [==============================] - 19s 28ms/step - loss: 7.4249e-04 - val_loss: 0.0012 - lr: 5.0000e-04\n",
      "Epoch 35/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 7.3245e-04 - val_loss: 0.0013 - lr: 5.0000e-04\n",
      "Epoch 36/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 7.2598e-04 - val_loss: 0.0013 - lr: 5.0000e-04\n",
      "Epoch 37/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 7.1578e-04 - val_loss: 0.0013 - lr: 5.0000e-04\n",
      "Epoch 38/1000\n",
      "660/660 [==============================] - 16s 25ms/step - loss: 7.0839e-04 - val_loss: 0.0013 - lr: 5.0000e-04\n",
      "Epoch 39/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 6.9914e-04 - val_loss: 0.0013 - lr: 5.0000e-04\n",
      "Epoch 40/1000\n",
      "660/660 [==============================] - 15s 22ms/step - loss: 6.9209e-04 - val_loss: 0.0013 - lr: 5.0000e-04\n",
      "Epoch 41/1000\n",
      "660/660 [==============================] - 15s 23ms/step - loss: 6.8421e-04 - val_loss: 0.0013 - lr: 5.0000e-04\n",
      "Epoch 42/1000\n",
      "660/660 [==============================] - 15s 22ms/step - loss: 6.7338e-04 - val_loss: 0.0013 - lr: 5.0000e-04\n",
      "Epoch 43/1000\n",
      "660/660 [==============================] - 15s 22ms/step - loss: 6.6898e-04 - val_loss: 0.0014 - lr: 5.0000e-04\n",
      "Epoch 44/1000\n",
      "660/660 [==============================] - 15s 22ms/step - loss: 6.6140e-04 - val_loss: 0.0013 - lr: 5.0000e-04\n",
      "Epoch 45/1000\n",
      "660/660 [==============================] - 15s 23ms/step - loss: 6.5603e-04 - val_loss: 0.0013 - lr: 5.0000e-04\n",
      "Epoch 46/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 6.4722e-04 - val_loss: 0.0014 - lr: 5.0000e-04\n",
      "Epoch 47/1000\n",
      "660/660 [==============================] - 15s 23ms/step - loss: 5.8145e-04 - val_loss: 0.0013 - lr: 5.0000e-05\n",
      "Epoch 48/1000\n",
      "660/660 [==============================] - 15s 23ms/step - loss: 5.6461e-04 - val_loss: 0.0014 - lr: 5.0000e-05\n",
      "Epoch 49/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 5.5722e-04 - val_loss: 0.0014 - lr: 5.0000e-05\n",
      "Epoch 50/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 5.5223e-04 - val_loss: 0.0014 - lr: 5.0000e-05\n",
      "Epoch 51/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 5.4792e-04 - val_loss: 0.0014 - lr: 5.0000e-05\n",
      "Epoch 52/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 5.4386e-04 - val_loss: 0.0014 - lr: 5.0000e-05\n",
      "Epoch 53/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 5.4090e-04 - val_loss: 0.0014 - lr: 5.0000e-05\n",
      "Epoch 54/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 5.3771e-04 - val_loss: 0.0014 - lr: 5.0000e-05\n",
      "Epoch 55/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 5.3489e-04 - val_loss: 0.0014 - lr: 5.0000e-05\n",
      "Epoch 56/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 5.3224e-04 - val_loss: 0.0014 - lr: 5.0000e-05\n",
      "Epoch 57/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 5.2981e-04 - val_loss: 0.0014 - lr: 5.0000e-05\n",
      "Epoch 58/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 5.1926e-04 - val_loss: 0.0014 - lr: 5.0000e-06\n",
      "Epoch 59/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 5.1835e-04 - val_loss: 0.0014 - lr: 5.0000e-06\n",
      "Epoch 60/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 5.1791e-04 - val_loss: 0.0014 - lr: 5.0000e-06\n",
      "Epoch 61/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 5.1757e-04 - val_loss: 0.0014 - lr: 5.0000e-06\n",
      "Epoch 62/1000\n",
      "660/660 [==============================] - 15s 23ms/step - loss: 5.1723e-04 - val_loss: 0.0014 - lr: 5.0000e-06\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = get_dataset('d6')\n",
    "model_medium.fit(x_train, y_train,\n",
    "          batch_size=2048,\n",
    "          epochs=1000,\n",
    "          verbose=1,\n",
    "          validation_split=0.1,\n",
    "          callbacks=[callbacks.ReduceLROnPlateau(monitor='loss', patience=10),\n",
    "                     callbacks.EarlyStopping(monitor='loss', patience=15, min_delta=1e-4)])\n",
    "\n",
    "model_medium.save('./engine_models/depth6_model.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HARD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 14, 8, 8)]        0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 64, 8, 8)          8128      \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 64, 8, 8)          36928     \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 64, 8, 8)          36928     \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 64, 8, 8)          36928     \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 64, 8, 8)          36928     \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                262208    \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 418,113\n",
      "Trainable params: 418,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_hard.compile(optimizer=optimizers.Adam(5e-4), loss='mean_squared_error')\n",
    "model_hard.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 0.0025 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 2/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 0.0021 - val_loss: 0.0020 - lr: 5.0000e-04\n",
      "Epoch 3/1000\n",
      "660/660 [==============================] - 15s 23ms/step - loss: 0.0020 - val_loss: 0.0020 - lr: 5.0000e-04\n",
      "Epoch 4/1000\n",
      "660/660 [==============================] - 14s 22ms/step - loss: 0.0019 - val_loss: 0.0019 - lr: 5.0000e-04\n",
      "Epoch 5/1000\n",
      "660/660 [==============================] - 14s 22ms/step - loss: 0.0019 - val_loss: 0.0019 - lr: 5.0000e-04\n",
      "Epoch 6/1000\n",
      "660/660 [==============================] - 14s 22ms/step - loss: 0.0019 - val_loss: 0.0020 - lr: 5.0000e-04\n",
      "Epoch 7/1000\n",
      "660/660 [==============================] - 14s 22ms/step - loss: 0.0019 - val_loss: 0.0019 - lr: 5.0000e-04\n",
      "Epoch 8/1000\n",
      "660/660 [==============================] - 15s 22ms/step - loss: 0.0019 - val_loss: 0.0020 - lr: 5.0000e-04\n",
      "Epoch 9/1000\n",
      "660/660 [==============================] - 15s 23ms/step - loss: 0.0018 - val_loss: 0.0019 - lr: 5.0000e-04\n",
      "Epoch 10/1000\n",
      "660/660 [==============================] - 15s 23ms/step - loss: 0.0018 - val_loss: 0.0019 - lr: 5.0000e-04\n",
      "Epoch 11/1000\n",
      "660/660 [==============================] - 14s 22ms/step - loss: 0.0018 - val_loss: 0.0019 - lr: 5.0000e-04\n",
      "Epoch 12/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 0.0018 - val_loss: 0.0020 - lr: 5.0000e-04\n",
      "Epoch 13/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 0.0017 - val_loss: 0.0019 - lr: 5.0000e-04\n",
      "Epoch 14/1000\n",
      "660/660 [==============================] - 15s 22ms/step - loss: 0.0017 - val_loss: 0.0019 - lr: 5.0000e-04\n",
      "Epoch 15/1000\n",
      "660/660 [==============================] - 14s 22ms/step - loss: 0.0017 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 16/1000\n",
      "660/660 [==============================] - 14s 22ms/step - loss: 0.0016 - val_loss: 0.0020 - lr: 5.0000e-04\n",
      "Epoch 17/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 0.0016 - val_loss: 0.0020 - lr: 5.0000e-04\n",
      "Epoch 18/1000\n",
      "660/660 [==============================] - 15s 22ms/step - loss: 0.0016 - val_loss: 0.0020 - lr: 5.0000e-04\n",
      "Epoch 19/1000\n",
      "660/660 [==============================] - 15s 23ms/step - loss: 0.0015 - val_loss: 0.0020 - lr: 5.0000e-04\n",
      "Epoch 20/1000\n",
      "660/660 [==============================] - 15s 23ms/step - loss: 0.0015 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 21/1000\n",
      "660/660 [==============================] - 15s 23ms/step - loss: 0.0014 - val_loss: 0.0020 - lr: 5.0000e-04\n",
      "Epoch 22/1000\n",
      "660/660 [==============================] - 15s 23ms/step - loss: 0.0014 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 23/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 0.0014 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 24/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 0.0014 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 25/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 0.0013 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 26/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 0.0013 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 27/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 0.0013 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 28/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 0.0013 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 29/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 0.0013 - val_loss: 0.0020 - lr: 5.0000e-04\n",
      "Epoch 30/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 0.0012 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 31/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 0.0012 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 32/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 0.0012 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 33/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 0.0012 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 34/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 0.0012 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 35/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 0.0012 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 36/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 0.0011 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 37/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 0.0011 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 38/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 0.0011 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 39/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 0.0011 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 40/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 0.0011 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 41/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 0.0011 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 42/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 0.0011 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 43/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 0.0010 - val_loss: 0.0024 - lr: 5.0000e-04\n",
      "Epoch 44/1000\n",
      "660/660 [==============================] - 15s 23ms/step - loss: 0.0010 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 45/1000\n",
      "660/660 [==============================] - 15s 23ms/step - loss: 0.0010 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 46/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 0.0010 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 47/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 0.0010 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 48/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 0.0010 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 49/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 9.8760e-04 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 50/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 9.8458e-04 - val_loss: 0.0024 - lr: 5.0000e-04\n",
      "Epoch 51/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 9.6575e-04 - val_loss: 0.0024 - lr: 5.0000e-04\n",
      "Epoch 52/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 9.6498e-04 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 53/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 9.5701e-04 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 54/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 9.4612e-04 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 55/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 9.4605e-04 - val_loss: 0.0024 - lr: 5.0000e-04\n",
      "Epoch 56/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 8.4323e-04 - val_loss: 0.0023 - lr: 5.0000e-05\n",
      "Epoch 57/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 8.1288e-04 - val_loss: 0.0023 - lr: 5.0000e-05\n",
      "Epoch 58/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 7.9856e-04 - val_loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 59/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 7.8877e-04 - val_loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 60/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 7.8110e-04 - val_loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 61/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 7.7442e-04 - val_loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 62/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 7.6852e-04 - val_loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 63/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 7.6355e-04 - val_loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 64/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 7.5895e-04 - val_loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 65/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 7.5476e-04 - val_loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 66/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 7.5107e-04 - val_loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 67/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 7.3546e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 68/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 7.3412e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 69/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 7.3347e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 70/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 7.3295e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 71/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 7.3239e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 72/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 7.3190e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 73/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 7.3143e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 74/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 7.3100e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 75/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 7.3051e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 76/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 7.3010e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 77/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 7.2966e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 78/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 7.2781e-04 - val_loss: 0.0024 - lr: 5.0000e-07\n",
      "Epoch 79/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 7.2769e-04 - val_loss: 0.0024 - lr: 5.0000e-07\n",
      "Epoch 80/1000\n",
      "660/660 [==============================] - 16s 24ms/step - loss: 7.2763e-04 - val_loss: 0.0024 - lr: 5.0000e-07\n",
      "Epoch 81/1000\n",
      "660/660 [==============================] - 15s 23ms/step - loss: 7.2758e-04 - val_loss: 0.0024 - lr: 5.0000e-07\n",
      "Epoch 82/1000\n",
      "660/660 [==============================] - 15s 23ms/step - loss: 7.2754e-04 - val_loss: 0.0024 - lr: 5.0000e-07\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = get_dataset('d10')\n",
    "model_hard.fit(x_train, y_train,\n",
    "          batch_size=2048,\n",
    "          epochs=1000,\n",
    "          verbose=1,\n",
    "          validation_split=0.1,\n",
    "          callbacks=[callbacks.ReduceLROnPlateau(monitor='loss', patience=10),\n",
    "                     callbacks.EarlyStopping(monitor='loss', patience=15, min_delta=1e-4)])\n",
    "\n",
    "model_hard.save('./engine_models/depth10_model.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RESIDUALS MODEL aka EXTREME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 14, 8, 8)]   0           []                               \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 64, 8, 8)     8128        ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 64, 8, 8)     36928       ['conv2d_15[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 64, 8, 8)    32          ['conv2d_16[0][0]']              \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 64, 8, 8)     0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 64, 8, 8)     36928       ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 64, 8, 8)    32          ['conv2d_17[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 64, 8, 8)     0           ['batch_normalization_1[0][0]',  \n",
      "                                                                  'conv2d_15[0][0]']              \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 64, 8, 8)     0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 64, 8, 8)     36928       ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 64, 8, 8)    32          ['conv2d_18[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 64, 8, 8)     0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 64, 8, 8)     36928       ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 64, 8, 8)    32          ['conv2d_19[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 64, 8, 8)     0           ['batch_normalization_3[0][0]',  \n",
      "                                                                  'activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 64, 8, 8)     0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 64, 8, 8)     36928       ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 64, 8, 8)    32          ['conv2d_20[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 64, 8, 8)     0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 64, 8, 8)     36928       ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 64, 8, 8)    32          ['conv2d_21[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 64, 8, 8)     0           ['batch_normalization_5[0][0]',  \n",
      "                                                                  'activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 64, 8, 8)     0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 64, 8, 8)     36928       ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 64, 8, 8)    32          ['conv2d_22[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 64, 8, 8)     0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 64, 8, 8)     36928       ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 64, 8, 8)    32          ['conv2d_23[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 64, 8, 8)     0           ['batch_normalization_7[0][0]',  \n",
      "                                                                  'activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 64, 8, 8)     0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 64, 8, 8)     36928       ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 64, 8, 8)    32          ['conv2d_24[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 64, 8, 8)     0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 64, 8, 8)     36928       ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 64, 8, 8)    32          ['conv2d_25[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 64, 8, 8)     0           ['batch_normalization_9[0][0]',  \n",
      "                                                                  'activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 64, 8, 8)     0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 4096)         0           ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 1)            4097        ['flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 381,825\n",
      "Trainable params: 381,665\n",
      "Non-trainable params: 160\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_re.compile(optimizer=optimizers.Adam(5e-4), loss='mean_squared_error')\n",
    "model_re.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "660/660 [==============================] - 54s 79ms/step - loss: 0.0012 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 2/1000\n",
      "660/660 [==============================] - 51s 78ms/step - loss: 0.0012 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 3/1000\n",
      "660/660 [==============================] - 52s 78ms/step - loss: 0.0012 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 4/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0012 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 5/1000\n",
      "660/660 [==============================] - 56s 85ms/step - loss: 0.0012 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 6/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0012 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 7/1000\n",
      "660/660 [==============================] - 58s 88ms/step - loss: 0.0011 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 8/1000\n",
      "660/660 [==============================] - 53s 81ms/step - loss: 0.0011 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 9/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0011 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 10/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0011 - val_loss: 0.0024 - lr: 5.0000e-04\n",
      "Epoch 11/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0011 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 12/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0011 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 13/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0011 - val_loss: 0.0024 - lr: 5.0000e-04\n",
      "Epoch 14/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0011 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 15/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0011 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 16/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0010 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 17/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0010 - val_loss: 0.0024 - lr: 5.0000e-04\n",
      "Epoch 18/1000\n",
      "660/660 [==============================] - 52s 78ms/step - loss: 0.0010 - val_loss: 0.0024 - lr: 5.0000e-04\n",
      "Epoch 19/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0010 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 20/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0010 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 21/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 9.9180e-04 - val_loss: 0.0024 - lr: 5.0000e-04\n",
      "Epoch 22/1000\n",
      "660/660 [==============================] - 52s 80ms/step - loss: 9.8513e-04 - val_loss: 0.0024 - lr: 5.0000e-04\n",
      "Epoch 23/1000\n",
      "660/660 [==============================] - 50s 76ms/step - loss: 9.6911e-04 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 24/1000\n",
      "660/660 [==============================] - 50s 76ms/step - loss: 9.6068e-04 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 25/1000\n",
      "660/660 [==============================] - 51s 77ms/step - loss: 9.5405e-04 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 26/1000\n",
      "660/660 [==============================] - 51s 77ms/step - loss: 9.4454e-04 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 27/1000\n",
      "660/660 [==============================] - 50s 76ms/step - loss: 9.3774e-04 - val_loss: 0.0024 - lr: 5.0000e-04\n",
      "Epoch 28/1000\n",
      "660/660 [==============================] - 51s 77ms/step - loss: 9.2431e-04 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 29/1000\n",
      "660/660 [==============================] - 178s 270ms/step - loss: 8.1576e-04 - val_loss: 0.0023 - lr: 5.0000e-05\n",
      "Epoch 30/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 7.8592e-04 - val_loss: 0.0023 - lr: 5.0000e-05\n",
      "Epoch 31/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 7.7312e-04 - val_loss: 0.0023 - lr: 5.0000e-05\n",
      "Epoch 32/1000\n",
      "660/660 [==============================] - 52s 78ms/step - loss: 7.6329e-04 - val_loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 33/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 7.5578e-04 - val_loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 34/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 7.4906e-04 - val_loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 35/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 7.4291e-04 - val_loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 36/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 7.3732e-04 - val_loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 37/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 7.3224e-04 - val_loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 38/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 7.2726e-04 - val_loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 39/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 7.2251e-04 - val_loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 40/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 7.0585e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 41/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 7.0436e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 42/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 7.0334e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 43/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 7.0270e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 44/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 7.0237e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 45/1000\n",
      "660/660 [==============================] - 54s 81ms/step - loss: 7.0158e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 46/1000\n",
      "660/660 [==============================] - 54s 81ms/step - loss: 7.0091e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 47/1000\n",
      "660/660 [==============================] - 54s 81ms/step - loss: 7.0059e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 48/1000\n",
      "660/660 [==============================] - 53s 81ms/step - loss: 6.9990e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 49/1000\n",
      "660/660 [==============================] - 54s 81ms/step - loss: 6.9947e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 50/1000\n",
      "660/660 [==============================] - 53s 81ms/step - loss: 6.9910e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 51/1000\n",
      "660/660 [==============================] - 53s 81ms/step - loss: 6.9726e-04 - val_loss: 0.0024 - lr: 5.0000e-07\n",
      "Epoch 52/1000\n",
      "660/660 [==============================] - 53s 81ms/step - loss: 6.9685e-04 - val_loss: 0.0024 - lr: 5.0000e-07\n",
      "Epoch 53/1000\n",
      "660/660 [==============================] - 54s 81ms/step - loss: 6.9672e-04 - val_loss: 0.0024 - lr: 5.0000e-07\n",
      "Epoch 54/1000\n",
      "660/660 [==============================] - 54s 81ms/step - loss: 6.9681e-04 - val_loss: 0.0024 - lr: 5.0000e-07\n",
      "Epoch 55/1000\n",
      "660/660 [==============================] - 53s 81ms/step - loss: 6.9673e-04 - val_loss: 0.0024 - lr: 5.0000e-07\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = get_dataset('d10')\n",
    "model_re.fit(x_train, y_train,\n",
    "          batch_size=2048,\n",
    "          epochs=1000,\n",
    "          verbose=1,\n",
    "          validation_split=0.1,\n",
    "          callbacks=[callbacks.ReduceLROnPlateau(monitor='loss', patience=10),\n",
    "                     callbacks.EarlyStopping(monitor='loss', patience=15, min_delta=1e-4)])\n",
    "\n",
    "model_re.save('./engine_models/depth10_model_residuals.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tiempo de ejecucion = 54 min 30 seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93eda801d5fd29625c3423228d2952e23befdd4fb970236ec85dd514a35765ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
