{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chess\n",
    "import chess.engine\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6600506300522068142\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6254755840\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 17637554184408226269\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 2070 SUPER, pci bus id: 0000:09:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comprobamos que la GPU está disponible\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.models as models\n",
    "import tensorflow.keras.layers as layers\n",
    "import tensorflow.keras.utils as utils\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "\n",
    "\n",
    "def build_model(conv_size, conv_depth):\n",
    "  board3d = layers.Input(shape=(14, 8, 8))\n",
    "\n",
    "  # adding the convolutional layers\n",
    "  x = board3d\n",
    "  for _ in range(conv_depth):\n",
    "    x = layers.Conv2D(filters=conv_size, kernel_size=3, padding='same', activation='relu', data_format='channels_first')(x)\n",
    "  x = layers.Flatten()(x)\n",
    "  x = layers.Dense(64, 'relu')(x)\n",
    "  x = layers.Dense(1, 'sigmoid')(x)\n",
    "\n",
    "  return models.Model(inputs=board3d, outputs=x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip connections (residual network) will likely improve the model for deeper connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_residual(conv_size, conv_depth):\n",
    "  board3d = layers.Input(shape=(14, 8, 8))\n",
    "\n",
    "  # adding the convolutional layers\n",
    "  x = layers.Conv2D(filters=conv_size, kernel_size=3, padding='same', data_format='channels_first')(board3d)\n",
    "  for _ in range(conv_depth):\n",
    "    previous = x\n",
    "    x = layers.Conv2D(filters=conv_size, kernel_size=3, padding='same', data_format='channels_first')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(filters=conv_size, kernel_size=3, padding='same', data_format='channels_first')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Add()([x, previous])\n",
    "    x = layers.Activation('relu')(x)\n",
    "  x = layers.Flatten()(x)\n",
    "  x = layers.Dense(1, 'sigmoid')(x)\n",
    "\n",
    "  return models.Model(inputs=board3d, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "model = build_model_residual(64, 5)\n",
    "utils.plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training ⬇️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500000, 14, 8, 8)\n",
      "(1500000,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.callbacks as callbacks\n",
    "\n",
    "\n",
    "def get_dataset():\n",
    "\tcontainer = np.load(r'C:\\Users\\Usuario\\Desktop\\Data Science Projects\\Chess_Engine_TFM\\data\\random_generated\\random_boards_d10.npz', allow_pickle=True)\n",
    "\tb, v, f = container['board_matrix'], container['eval'], container['board_fen']\n",
    "\tv[v == None] = 0\n",
    "\tv = np.asarray(v / abs(v).max() / 2 + 0.5, dtype=np.float32) # normalization (0 - 1)\n",
    "\treturn b, v\n",
    "\n",
    "\n",
    "x_train, y_train = get_dataset()\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 14, 8, 8)]   0           []                               \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 64, 8, 8)     8128        ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 64, 8, 8)     36928       ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 64, 8, 8)    32          ['conv2d_10[0][0]']              \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 64, 8, 8)     0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)             (None, 64, 8, 8)     36928       ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 64, 8, 8)    32          ['conv2d_11[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 64, 8, 8)     0           ['batch_normalization_1[0][0]',  \n",
      "                                                                  'conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 64, 8, 8)     0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)             (None, 64, 8, 8)     36928       ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 64, 8, 8)    32          ['conv2d_12[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 64, 8, 8)     0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)             (None, 64, 8, 8)     36928       ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 64, 8, 8)    32          ['conv2d_13[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 64, 8, 8)     0           ['batch_normalization_3[0][0]',  \n",
      "                                                                  'activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 64, 8, 8)     0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)             (None, 64, 8, 8)     36928       ['activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 64, 8, 8)    32          ['conv2d_14[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 64, 8, 8)     0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)             (None, 64, 8, 8)     36928       ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 64, 8, 8)    32          ['conv2d_15[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 64, 8, 8)     0           ['batch_normalization_5[0][0]',  \n",
      "                                                                  'activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 64, 8, 8)     0           ['add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)             (None, 64, 8, 8)     36928       ['activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 64, 8, 8)    32          ['conv2d_16[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_6 (Activation)      (None, 64, 8, 8)     0           ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)             (None, 64, 8, 8)     36928       ['activation_6[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 64, 8, 8)    32          ['conv2d_17[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 64, 8, 8)     0           ['batch_normalization_7[0][0]',  \n",
      "                                                                  'activation_5[0][0]']           \n",
      "                                                                                                  \n",
      " activation_7 (Activation)      (None, 64, 8, 8)     0           ['add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)             (None, 64, 8, 8)     36928       ['activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 64, 8, 8)    32          ['conv2d_18[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_8 (Activation)      (None, 64, 8, 8)     0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 64, 8, 8)     36928       ['activation_8[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 64, 8, 8)    32          ['conv2d_19[0][0]']              \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 64, 8, 8)     0           ['batch_normalization_9[0][0]',  \n",
      "                                                                  'activation_7[0][0]']           \n",
      "                                                                                                  \n",
      " activation_9 (Activation)      (None, 64, 8, 8)     0           ['add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 4096)         0           ['activation_9[0][0]']           \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 1)            4097        ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 381,825\n",
      "Trainable params: 381,665\n",
      "Non-trainable params: 160\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=optimizers.Adam(5e-4), loss='mean_squared_error')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "660/660 [==============================] - 22s 23ms/step - loss: 0.0026 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 2/1000\n",
      "660/660 [==============================] - 15s 22ms/step - loss: 0.0021 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 3/1000\n",
      "660/660 [==============================] - 15s 23ms/step - loss: 0.0020 - val_loss: 0.0020 - lr: 5.0000e-04\n",
      "Epoch 4/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 0.0020 - val_loss: 0.0019 - lr: 5.0000e-04\n",
      "Epoch 5/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0019 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 6/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0019 - val_loss: 0.0020 - lr: 5.0000e-04\n",
      "Epoch 7/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0019 - val_loss: 0.0019 - lr: 5.0000e-04\n",
      "Epoch 8/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0019 - val_loss: 0.0020 - lr: 5.0000e-04\n",
      "Epoch 9/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0018 - val_loss: 0.0019 - lr: 5.0000e-04\n",
      "Epoch 10/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 0.0018 - val_loss: 0.0020 - lr: 5.0000e-04\n",
      "Epoch 11/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 0.0018 - val_loss: 0.0019 - lr: 5.0000e-04\n",
      "Epoch 12/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0018 - val_loss: 0.0019 - lr: 5.0000e-04\n",
      "Epoch 13/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 0.0017 - val_loss: 0.0020 - lr: 5.0000e-04\n",
      "Epoch 14/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 0.0017 - val_loss: 0.0020 - lr: 5.0000e-04\n",
      "Epoch 15/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 0.0017 - val_loss: 0.0019 - lr: 5.0000e-04\n",
      "Epoch 16/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 0.0016 - val_loss: 0.0020 - lr: 5.0000e-04\n",
      "Epoch 17/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 0.0016 - val_loss: 0.0020 - lr: 5.0000e-04\n",
      "Epoch 18/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0015 - val_loss: 0.0020 - lr: 5.0000e-04\n",
      "Epoch 19/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0015 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 20/1000\n",
      "660/660 [==============================] - 18s 28ms/step - loss: 0.0015 - val_loss: 0.0020 - lr: 5.0000e-04\n",
      "Epoch 21/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0014 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 22/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0014 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 23/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0014 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 24/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0013 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 25/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0013 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 26/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0013 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 27/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0013 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 28/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0013 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 29/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0012 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 30/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0012 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 31/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0012 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 32/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0012 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 33/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0012 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 34/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0012 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 35/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0011 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 36/1000\n",
      "660/660 [==============================] - 18s 28ms/step - loss: 0.0011 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 37/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0011 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 38/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0011 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 39/1000\n",
      "660/660 [==============================] - 18s 28ms/step - loss: 0.0011 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 40/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0011 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 41/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0011 - val_loss: 0.0025 - lr: 5.0000e-04\n",
      "Epoch 42/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0010 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 43/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0010 - val_loss: 0.0024 - lr: 5.0000e-04\n",
      "Epoch 44/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0010 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 45/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 0.0010 - val_loss: 0.0024 - lr: 5.0000e-04\n",
      "Epoch 46/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 9.9918e-04 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 47/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 9.8565e-04 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 48/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 9.7248e-04 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 49/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 9.6457e-04 - val_loss: 0.0024 - lr: 5.0000e-04\n",
      "Epoch 50/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 9.5966e-04 - val_loss: 0.0024 - lr: 5.0000e-04\n",
      "Epoch 51/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 9.4181e-04 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 52/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 9.3820e-04 - val_loss: 0.0024 - lr: 5.0000e-04\n",
      "Epoch 53/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 9.2833e-04 - val_loss: 0.0024 - lr: 5.0000e-04\n",
      "Epoch 54/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 9.2167e-04 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 55/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 9.1597e-04 - val_loss: 0.0024 - lr: 5.0000e-04\n",
      "Epoch 56/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 9.0496e-04 - val_loss: 0.0024 - lr: 5.0000e-04\n",
      "Epoch 57/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 8.9156e-04 - val_loss: 0.0024 - lr: 5.0000e-04\n",
      "Epoch 58/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 8.9417e-04 - val_loss: 0.0024 - lr: 5.0000e-04\n",
      "Epoch 59/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 7.9373e-04 - val_loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 60/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 7.6458e-04 - val_loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 61/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 7.5091e-04 - val_loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 62/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 7.4186e-04 - val_loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 63/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 7.3445e-04 - val_loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 64/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 7.2898e-04 - val_loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 65/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 7.2322e-04 - val_loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 66/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 7.1868e-04 - val_loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 67/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 7.1427e-04 - val_loss: 0.0025 - lr: 5.0000e-05\n",
      "Epoch 68/1000\n",
      "660/660 [==============================] - 17s 26ms/step - loss: 7.1086e-04 - val_loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 69/1000\n",
      "660/660 [==============================] - 18s 28ms/step - loss: 7.0715e-04 - val_loss: 0.0025 - lr: 5.0000e-05\n",
      "Epoch 70/1000\n",
      "660/660 [==============================] - 18s 28ms/step - loss: 6.9247e-04 - val_loss: 0.0025 - lr: 5.0000e-06\n",
      "Epoch 71/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 6.9121e-04 - val_loss: 0.0025 - lr: 5.0000e-06\n",
      "Epoch 72/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 6.9058e-04 - val_loss: 0.0025 - lr: 5.0000e-06\n",
      "Epoch 73/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 6.9009e-04 - val_loss: 0.0025 - lr: 5.0000e-06\n",
      "Epoch 74/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 6.8965e-04 - val_loss: 0.0025 - lr: 5.0000e-06\n",
      "Epoch 75/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 6.8917e-04 - val_loss: 0.0025 - lr: 5.0000e-06\n",
      "Epoch 76/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 6.8875e-04 - val_loss: 0.0025 - lr: 5.0000e-06\n",
      "Epoch 77/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 6.8834e-04 - val_loss: 0.0025 - lr: 5.0000e-06\n",
      "Epoch 78/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 6.8799e-04 - val_loss: 0.0025 - lr: 5.0000e-06\n",
      "Epoch 79/1000\n",
      "660/660 [==============================] - 18s 28ms/step - loss: 6.8749e-04 - val_loss: 0.0025 - lr: 5.0000e-06\n",
      "Epoch 80/1000\n",
      "660/660 [==============================] - 18s 28ms/step - loss: 6.8715e-04 - val_loss: 0.0025 - lr: 5.0000e-06\n",
      "Epoch 81/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 6.8534e-04 - val_loss: 0.0025 - lr: 5.0000e-07\n",
      "Epoch 82/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 6.8525e-04 - val_loss: 0.0025 - lr: 5.0000e-07\n",
      "Epoch 83/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 6.8520e-04 - val_loss: 0.0025 - lr: 5.0000e-07\n",
      "Epoch 84/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 6.8514e-04 - val_loss: 0.0025 - lr: 5.0000e-07\n",
      "Epoch 85/1000\n",
      "660/660 [==============================] - 18s 27ms/step - loss: 6.8511e-04 - val_loss: 0.0025 - lr: 5.0000e-07\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=2048,\n",
    "          epochs=1000,\n",
    "          verbose=1,\n",
    "          validation_split=0.1,\n",
    "          callbacks=[callbacks.ReduceLROnPlateau(monitor='loss', patience=10),\n",
    "                     callbacks.EarlyStopping(monitor='loss', patience=15, min_delta=1e-4)])\n",
    "\n",
    "model.save('./engine_models/depth10_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "660/660 [==============================] - 54s 79ms/step - loss: 0.0311 - val_loss: 0.0052 - lr: 5.0000e-04\n",
      "Epoch 2/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0036 - val_loss: 0.0029 - lr: 5.0000e-04\n",
      "Epoch 3/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0028 - val_loss: 0.0024 - lr: 5.0000e-04\n",
      "Epoch 4/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0025 - val_loss: 0.0024 - lr: 5.0000e-04\n",
      "Epoch 5/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0024 - val_loss: 0.0027 - lr: 5.0000e-04\n",
      "Epoch 6/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0024 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 7/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0023 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 8/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0023 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 9/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0022 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 10/1000\n",
      "660/660 [==============================] - 53s 81ms/step - loss: 0.0022 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 11/1000\n",
      "660/660 [==============================] - 53s 81ms/step - loss: 0.0021 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 12/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0021 - val_loss: 0.0020 - lr: 5.0000e-04\n",
      "Epoch 13/1000\n",
      "660/660 [==============================] - 52s 78ms/step - loss: 0.0020 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 14/1000\n",
      "660/660 [==============================] - 52s 78ms/step - loss: 0.0020 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 15/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0020 - val_loss: 0.0020 - lr: 5.0000e-04\n",
      "Epoch 16/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0020 - val_loss: 0.0020 - lr: 5.0000e-04\n",
      "Epoch 17/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0019 - val_loss: 0.0020 - lr: 5.0000e-04\n",
      "Epoch 18/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0019 - val_loss: 0.0035 - lr: 5.0000e-04\n",
      "Epoch 19/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0019 - val_loss: 0.0019 - lr: 5.0000e-04\n",
      "Epoch 20/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0019 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 21/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0019 - val_loss: 0.0019 - lr: 5.0000e-04\n",
      "Epoch 22/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0019 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 23/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0018 - val_loss: 0.0019 - lr: 5.0000e-04\n",
      "Epoch 24/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0018 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 25/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0018 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 26/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0018 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 27/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0018 - val_loss: 0.0019 - lr: 5.0000e-04\n",
      "Epoch 28/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0018 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 29/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0018 - val_loss: 0.0019 - lr: 5.0000e-04\n",
      "Epoch 30/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0018 - val_loss: 0.0019 - lr: 5.0000e-04\n",
      "Epoch 31/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0017 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 32/1000\n",
      "660/660 [==============================] - 52s 78ms/step - loss: 0.0017 - val_loss: 0.0024 - lr: 5.0000e-04\n",
      "Epoch 33/1000\n",
      "660/660 [==============================] - 52s 78ms/step - loss: 0.0017 - val_loss: 0.0020 - lr: 5.0000e-04\n",
      "Epoch 34/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0017 - val_loss: 0.0020 - lr: 5.0000e-04\n",
      "Epoch 35/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0017 - val_loss: 0.0019 - lr: 5.0000e-04\n",
      "Epoch 36/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0016 - val_loss: 0.0020 - lr: 5.0000e-04\n",
      "Epoch 37/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0016 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 38/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0016 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 39/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0016 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 40/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0015 - val_loss: 0.0029 - lr: 5.0000e-04\n",
      "Epoch 41/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0015 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 42/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0015 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 43/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0015 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 44/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0014 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 45/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0014 - val_loss: 0.0024 - lr: 5.0000e-04\n",
      "Epoch 46/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0014 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 47/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0014 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 48/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0014 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 49/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0013 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 50/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0013 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 51/1000\n",
      "660/660 [==============================] - 52s 78ms/step - loss: 0.0013 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 52/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0013 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 53/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0013 - val_loss: 0.0026 - lr: 5.0000e-04\n",
      "Epoch 54/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0012 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 55/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0012 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 56/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0012 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 57/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0012 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 58/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0012 - val_loss: 0.0021 - lr: 5.0000e-04\n",
      "Epoch 59/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0012 - val_loss: 0.0024 - lr: 5.0000e-04\n",
      "Epoch 60/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0012 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 61/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0011 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 62/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0011 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 63/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0011 - val_loss: 0.0024 - lr: 5.0000e-04\n",
      "Epoch 64/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0011 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 65/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0011 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 66/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0011 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 67/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0011 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 68/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0010 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 69/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 0.0010 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 70/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0010 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 71/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0010 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 72/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 0.0010 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 73/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 9.9956e-04 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 74/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 9.9056e-04 - val_loss: 0.0022 - lr: 5.0000e-04\n",
      "Epoch 75/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 9.8828e-04 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 76/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 9.7328e-04 - val_loss: 0.0025 - lr: 5.0000e-04\n",
      "Epoch 77/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 9.6588e-04 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 78/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 9.5270e-04 - val_loss: 0.0023 - lr: 5.0000e-04\n",
      "Epoch 79/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 8.4555e-04 - val_loss: 0.0023 - lr: 5.0000e-05\n",
      "Epoch 80/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 8.1498e-04 - val_loss: 0.0023 - lr: 5.0000e-05\n",
      "Epoch 81/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 8.0173e-04 - val_loss: 0.0023 - lr: 5.0000e-05\n",
      "Epoch 82/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 7.9226e-04 - val_loss: 0.0023 - lr: 5.0000e-05\n",
      "Epoch 83/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 7.8446e-04 - val_loss: 0.0023 - lr: 5.0000e-05\n",
      "Epoch 84/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 7.7782e-04 - val_loss: 0.0023 - lr: 5.0000e-05\n",
      "Epoch 85/1000\n",
      "660/660 [==============================] - 52s 78ms/step - loss: 7.7122e-04 - val_loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 86/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 7.6566e-04 - val_loss: 0.0023 - lr: 5.0000e-05\n",
      "Epoch 87/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 7.6086e-04 - val_loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 88/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 7.5552e-04 - val_loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 89/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 7.5123e-04 - val_loss: 0.0024 - lr: 5.0000e-05\n",
      "Epoch 90/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 7.3468e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 91/1000\n",
      "660/660 [==============================] - 54s 81ms/step - loss: 7.3276e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 92/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 7.3218e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 93/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 7.3144e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 94/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 7.3072e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 95/1000\n",
      "660/660 [==============================] - 52s 78ms/step - loss: 7.3014e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 96/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 7.2986e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 97/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 7.2929e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 98/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 7.2858e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 99/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 7.2811e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 100/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 7.2756e-04 - val_loss: 0.0024 - lr: 5.0000e-06\n",
      "Epoch 101/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 7.2576e-04 - val_loss: 0.0024 - lr: 5.0000e-07\n",
      "Epoch 102/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 7.2534e-04 - val_loss: 0.0024 - lr: 5.0000e-07\n",
      "Epoch 103/1000\n",
      "660/660 [==============================] - 53s 80ms/step - loss: 7.2555e-04 - val_loss: 0.0024 - lr: 5.0000e-07\n",
      "Epoch 104/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 7.2538e-04 - val_loss: 0.0024 - lr: 5.0000e-07\n",
      "Epoch 105/1000\n",
      "660/660 [==============================] - 52s 79ms/step - loss: 7.2520e-04 - val_loss: 0.0024 - lr: 5.0000e-07\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=2048,\n",
    "          epochs=1000,\n",
    "          verbose=1,\n",
    "          validation_split=0.1,\n",
    "          callbacks=[callbacks.ReduceLROnPlateau(monitor='loss', patience=10),\n",
    "                     callbacks.EarlyStopping(monitor='loss', patience=15, min_delta=1e-4)])\n",
    "\n",
    "model.save('./engine_models/depth10_model_residuals.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93eda801d5fd29625c3423228d2952e23befdd4fb970236ec85dd514a35765ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
